{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilNfza8CpYnU"
   },
   "source": [
    "# INTELIGENCIA ARTIFICIAL: EB Ejercicios Tema 5: REDES NEURONALES \n",
    "## EJERCICIO: ¿Cómo calcular el número óptimo de neuronas en la capa oculta?\n",
    "**Implementar** una red neuronal para clasificar emails en SPAM/no SPAM con una única capa oculta. El número de neuronas de la capa oculta se debe calcular de forma óptima con una búsqueda grid usando para ello un conjunto de validación. Use como método de evaluación holdout, donde el conjunto de entrenamiento está formado por el 70% de las primeras filas del conjunto de datos, y el conjunto de test por el resto. \n",
    "\n",
    "Después de un preprocesado de los emails, cada email se codifica como un vector de 1899 elementos, donde cada elemento es una característica concreta del email. Todos los emails se encuentran repartidos en los ficheros **spamTrain.mat**, **spamVal.mat** y **spamTest.mat**, en los que tenéis la matriz de atributos **X** y el vector de clase **y** correspondiente al conjunto de training, de validación y de test, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA1FENuDrZ8V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU5E_gnL-RC4"
   },
   "source": [
    "# A) DECLARACIÓN DE FUNCIONES Y CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB6G7rqop1sk"
   },
   "source": [
    "## 1) Cargar los datos de entrada\n",
    "Estos datos están almacenados en ficheros spamTrain.mat, spamVal.mat y spamTest.mat. \n",
    "Visualizar las dimensiones de la matriz de atributos y del vector clase para cada conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjyTyqjLpzQF",
    "outputId": "c7671f89-33f2-4e79-c293-a46ff2f57f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ...\n",
      "\n",
      "El tamaño de X_train es:  3000  filas y  1899  columnas.\n",
      "El tamaño de y_train es:  3000  filas y  1  columnas. \n",
      "El tamaño de X_val es:  1000  filas y  1899  columnas.\n",
      "El tamaño de y_val es:  1000  filas y  1  columnas. \n",
      "El tamaño de X_test es:  1000  filas y  1899  columnas.\n",
      "El tamaño de y_test es:  1000  filas y  1  columnas. \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data ...\\n\")\n",
    "\n",
    "# Datos de entrenamiento\n",
    "data_train = sio.loadmat(\"spamTrain.mat\") \n",
    "X_train = data_train['X']\n",
    "y_train = data_train['y']\n",
    "print(\"El tamaño de X_train es: \", X_train.shape[0], \" filas y \", X_train.shape[1], \" columnas.\")\n",
    "print(\"El tamaño de y_train es: \", y_train.shape[0], \" filas y \", y_train.shape[1], \" columnas. \")\n",
    "\n",
    "# Datos de validación\n",
    "data_val = sio.loadmat(\"spamValidation.mat\") \n",
    "X_val = data_val['X']\n",
    "y_val = data_val['y']\n",
    "print(\"El tamaño de X_val es: \", X_val.shape[0], \" filas y \", X_val.shape[1], \" columnas.\")\n",
    "print(\"El tamaño de y_val es: \", y_val.shape[0], \" filas y \", y_val.shape[1], \" columnas. \")\n",
    "\n",
    "# Datos de test\n",
    "data_test = sio.loadmat(\"spamTest.mat\") \n",
    "X_test = data_test['Xtest']\n",
    "y_test = data_test['ytest']\n",
    "print(\"El tamaño de X_test es: \", X_test.shape[0], \" filas y \", X_test.shape[1], \" columnas.\")\n",
    "print(\"El tamaño de y_test es: \", y_test.shape[0], \" filas y \", y_test.shape[1], \" columnas. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_QxZU8psVIV"
   },
   "source": [
    "## 2) Hipótesis y función coste\n",
    "El objetivo de la red neuronal es minimizar la función de coste definida por: \n",
    "\\begin{equation}\n",
    "J(\\Theta)=\\frac{-1}{m}\\sum_{i=1}^{m} \\sum_{k=1}^{K} (y_k^i \\cdot log((h_{\\theta}(x^i))_k)+(1-y_k^i)\\cdot log(1-(h_{\\theta}(x^i))_k))\n",
    "\\end{equation}\n",
    "\n",
    "Con el objeto de usar funciones de optimización avanzada de Python, implementar una función que devuelva el valor del coste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPREa9_8tVJZ"
   },
   "outputs": [],
   "source": [
    "# Función sigmoide tradicional\n",
    "def sigmoid(z):\n",
    "  g = 1 / (1+np.exp(-z))\n",
    "  return g\n",
    "\n",
    "# Función propagación hacia delante o forward propagation\n",
    "def forward(theta1, theta2, X, i):\n",
    "  # bias  + neuronas de la capa 1\n",
    "  a1 = np.hstack((1, X[i])) # Igual que: ones = np.ones(1)  a1 = np.hstack((ones, X[i]))\n",
    "  a2 = sigmoid(theta1 @ a1 )\n",
    "\n",
    "  # bias + neuronas de la capa 2\n",
    "  a2 = np.hstack((1, a2)) # Igual que: a2 = np.hstack((ones, a2))\n",
    "  \n",
    "  # a3 es la salida de la capa 3 o hipótesis (h)\n",
    "  a3 = sigmoid(theta2 @ a2)\n",
    "  return a1, a2, a3\n",
    "\n",
    "# Función coste sin regularizar para redes neuronales\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "  # Paso 1: Enrollar nn_params para obtener cada uno de los theta (pesos/parámetros)\n",
    "  theta1 = np.reshape(a = nn_params[:hidden_layer_size*(input_layer_size+1)], # datos \n",
    "                      newshape = (hidden_layer_size, input_layer_size+1), # nueva shape: (hidden_layer_size, input_layer_size+1)\n",
    "                      order = 'F') # El order debe ser el mismo que cuando desenrollemos\n",
    "  theta2 = np.reshape(a = nn_params[hidden_layer_size*(input_layer_size+1):],\n",
    "                      newshape = (num_labels, hidden_layer_size+1), \n",
    "                      order = 'F')\n",
    "  \n",
    "  # Paso 2: Definir variables necesarias\n",
    "  m = len(y) \n",
    "  suma = 0 \n",
    "  y_d = pd.DataFrame(y) # ¡¡IMPORTANTE!!: No aplicamos one-hot encoding (y_d = pd.get_dummies(y.flatten())) ya que solo tenemos 1 clase: spam/no spam. \n",
    "  # Pero es importante transformar y a DataFrame para poder acceder fila por fila\n",
    "\n",
    "  # Paso 3: Para cada fila \n",
    "  for i in range(X.shape[0]):\n",
    "      # Paso 3.1: Forward propagation\n",
    "      a1, a2, h = forward(theta1, theta2, X, i)\n",
    "      # Paso 3.2: Coste (como en regresión logística)\n",
    "      temp1 = y_d.iloc[i] * (np.log(h)) \n",
    "      temp2 = (1 - y_d.iloc[i]) * np.log(1 - h) \n",
    "      temp3 = np.sum(temp1 + temp2) \n",
    "      suma = suma + temp3\n",
    "  J = (np.sum(suma) / (-m))\n",
    "  return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94KvljhWsvMw"
   },
   "source": [
    "## 3) Gradiente\n",
    "Para poder usar funciones de optimización avanzadas de Python, implementar una función que devuelva el valor del gradiente formado por las derivadas parciales. \n",
    "Recordar que las derivadas parciales se calculan con el algoritmo de Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LxexTmOt1KB"
   },
   "outputs": [],
   "source": [
    "def nnGradFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "  # Paso 1: Enrollar nn_params para obtener cada uno de los theta (pesos/parámetros)\n",
    "  initial_theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                              (hidden_layer_size, input_layer_size + 1), 'F')\n",
    "  initial_theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):],\n",
    "                              (num_labels, hidden_layer_size + 1), 'F')\n",
    "  \n",
    "  # Paso 2: Definir variables necesarias\n",
    "  m = len(y)\n",
    "  y_d = pd.DataFrame(y) # ¡¡IMPORTANTE!!: No aplicamos one-hot encoding (y_d = pd.get_dummies(y.flatten())) ya que solo tenemos 1 clase: spam/no spam. \n",
    "  # Pero es importante transformar y a DataFrame para poder acceder fila por fila\n",
    "  delta1 = np.zeros(initial_theta1.shape) # Delta1 tendrá las mismas dimensiones que initial_theta1\n",
    "  delta2 = np.zeros(initial_theta2.shape) # Delta2 tendrá las mismas dimensiones que initial_theta2\n",
    "\n",
    "  # Paso 3: Para cada fila\n",
    "  for i in range(X.shape[0]):\n",
    "      # Paso 3.1: Forward propagation\n",
    "      a1, a2, a3 = forward(initial_theta1, initial_theta2, X, i)\n",
    "      # Paso 3.2: Cálculo de los delta/errores (capa 1 no tiene)\n",
    "      d3 = a3 - y_d.iloc[i] # última capa \n",
    "      d2 = np.multiply(np.dot(initial_theta2.T,d3), np.multiply(a2, 1-a2)) # capa 2\n",
    "      # Paso 3.3: Cálculo de las derivadas ajustando las dimensiones de los errores y las activaciones de cada capa correctamente\n",
    "      delta1 = delta1 + (np.reshape(d2[1:,],(hidden_layer_size, 1)) @ np.reshape(a1, (1, input_layer_size+1))) # IGUAL: delta1 = delta1 + d2[1:,np.newaxis] @ a1[np.newaxis, :] \n",
    "      delta2 = delta2 + (np.reshape(d3.values, (num_labels, 1)) @ np.reshape(a2, (1, hidden_layer_size+1))) # IGUAL: delta2 = delta2 + d3[:,np.newaxis] @ a2[np.newaxis, :] \n",
    "\n",
    "  # Paso 4: Se desenrollan ambas derivadas con el mismo order con el que se enrollaron\n",
    "  delta1 /= m\n",
    "  delta2 /= m\n",
    "  gradiente = np.hstack((delta1.ravel(order='F'), delta2.ravel(order='F')))\n",
    "  return gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxagKJQ-usRu"
   },
   "source": [
    "## 4) Función para entrenar la red neuronal usando optimizador avanzado\n",
    "Implementar una función llamada training que sea la encargada de realizar el entrenamiento de una red neuronal. Para ello, esta función recibirá los parámetros theta iniciales, el tamaño de la capa de entrada, el tamaño de la capa oculta, el número de clases y el conjunto con el que se realice el entrenamiento (X e y). Devolverá los parámetros theta óptimos que definen el modelo aprendido.  \n",
    "\n",
    "Se recomienda el uso de la función fmin_cg de la librería scipy.optimize. Esta función tiene como argumentos de salida el vector nn_params que contiene los pesos óptimos Theta1 y Theta 2 en forma de vector y el valor J alcanzado. \n",
    "\n",
    "Recordar que a partir del vector nn_params hay que reconstruir los pesos Theta1 y Theta2 con las dimensiones adecuadas, usando la función de Python reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EukrtIovajs"
   },
   "outputs": [],
   "source": [
    "def training(initial_theta1, initial_theta2, X_train, y_train, input_layer_size, hidden_layer_size, num_labels):\n",
    "  maxiter = 30 # Si tarda demasiado, se puede bajar el número de iteraciones al hacer la prueba inicial para comprobar que el entrenamiento es el adecuado\n",
    "\n",
    "  # Paso 1: Desenrollar los parámetros con el mismo order con el que se enrollaron\n",
    "  nn_initial_params = np.hstack((initial_theta1.ravel(order='F'), initial_theta2.ravel(order='F')))\n",
    "\n",
    "  # Paso 2: Llamada al optimizador avanzado gradiente conjugado con la función: fmin_cg\n",
    "  nn_params = opt.fmin_cg(maxiter=maxiter, f=nnCostFunction, x0=nn_initial_params, fprime=nnGradFunction,\n",
    "                        args=(input_layer_size, hidden_layer_size, num_labels, X_train, y_train.flatten()))\n",
    "  \n",
    "  # Paso 3: Enrollar los pesos/parámetros theta1 y theta2 desde la salida del optimizador avanzado (nn_params)\n",
    "  theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                      (hidden_layer_size, input_layer_size + 1), order = 'F')\n",
    "  theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \n",
    "                      (num_labels, hidden_layer_size + 1), order = 'F')\n",
    "  return theta1, theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DXPJU5tEH1"
   },
   "source": [
    "## 5) Función para predecir\n",
    "Implementar una función para predecir si un conjunto de emails son SPAM/no SPAM (binario).\n",
    "\n",
    "Podrías usar directamente la función forward (fila por fila) y añadir la última línea que calcula la predicción {0,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wf6tUGkKpQEH"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "def predict(theta1, theta2, X):\n",
    "  # Variables útiles\n",
    "  m = len(X)\n",
    "  ones = np.ones((m,1))\n",
    "\n",
    "  a1 = np.hstack((ones, X))\n",
    "  a2 = sigmoid(a1 @ theta1.T)\n",
    "  a2 = np.hstack((ones, a2))\n",
    "  h = sigmoid(a2 @ theta2.T) # La hipótesis o predicción \n",
    "\n",
    "  # Si h es mayor o igual que 0.5, entonces la predicción será 1, si es menor que 0.5 será 0\n",
    "  # Función np.where: https://numpy.org/doc/stable/reference/generated/numpy.where.html : np.where(condicion, value if TRUE, value if FALSE)\n",
    "  pred = np.where(... , ... , ....)\n",
    "  return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC3mWPXCwWeF"
   },
   "source": [
    "## 6) Cálculo del número óptimo de neuronas de la capa oculta con búsqueda grid usando el conjunto de validación.\n",
    "La función optimalHiddenNeurons se encarga de calcular el número óptimo de neuronas en la capa oculta con una búsqueda grid usando un conjunto de validación. \n",
    "\n",
    "Para ello, esta función recibirá el tamaño de la capa de entrada, el conjunto con el que se realice el entrenamiento, y el conjunto con el que se calcule la tasa de acierto, y devolverá el número óptimo de neuronas. El número óptimo de neuronas será aquel que haga máxima la tasa de acierto. \n",
    "\n",
    "El grid estará formado por el conjunto de neuronas {1,2,3,4,5,6,7,8,9,10}.\n",
    "\n",
    "En esta función se imprimirá la tasa de acierto para el grid de neuronas, el número óptimo de neuronas y la tasa de acierto máxima alcanzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGcWfufO7aMC"
   },
   "source": [
    "### 6.1) Función para inicializar los pesos aleatoriamente de una capa \n",
    "Esta función recibe como parámetro el peso de una capa con L_in neuronas de entrada y L_out neuronas de salida. [EB TEMA 5 PARTE II ÚLTIMA DIAPOSITIVA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWwOS7U4zTWK"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "  # Variable a devolver tendrá dimensiones: (L_out, L_in+1) # +1 procedente de la bias\n",
    "  W = np.zeros((L_out, 1+L_in))\n",
    "\n",
    "  # Se va a inicializar W de manera random para \"romper\" la simetría mientras se entrena la red neuronal\n",
    "  epsilon_init = 0.12 # Se define un epsilon\n",
    "  W = ....\n",
    "  return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByNWcT0vyaXf"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "def optimalHiddenNeurons(input_layer_size, num_labels, X_train, y_train, X_val, y_val):\n",
    "  # Paso 1: # Inicializamos variables útiles\n",
    "  num_max_neuronas = 10 # el número máximo de neuronas en el grid\n",
    "  print('\\nCalculando número óptimo de neuronas de la capa oculta... \\n')\n",
    "  print('\\nNúmero máximo de neuronas: ',num_max_neuronas)\n",
    "\n",
    "  arr_accuracy = [] # Inicializamos la lista donde almacenaremos la precisión del conjunto de \n",
    "  # validación para los diferentes números de neuronas del grid\n",
    "\n",
    "  # Paso 2: Bucle desde 1 hasta num_max_neuronas (incluido, por eso el +1)\n",
    "  for hidden_layer_size in range(1, num_max_neuronas+1):\n",
    "    print('-----\\nNúmero de neuronas de la capa oculta: ',hidden_layer_size)\n",
    "\n",
    "    # Paso 2.1: Inicializar los pesos aleatoriamente con las dimensiones correctas\n",
    "    initial_theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "    initial_theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "    # Paso 2.2: Entrenamiento de la red neuronal con el número de neuronas de la capa oculta\n",
    "    theta1_opt, theta2_opt = training(initial_theta1, initial_theta2, X_train, y_train, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "    # Paso 2.3: Predicción usando el conjunto de validación\n",
    "    pred = predict(... , ...., ....)\n",
    "\n",
    "    # Paso 2.4: Calcular la precisión/accuracy máxima\n",
    "    arr_accuracy.append(np.mean(pred== ... )) # Se añade a la lista de precisión\n",
    "    print(\"accuracy: \", np.mean(pred== ... ))\n",
    "    \n",
    "  # Paso 3: Fuera del bucle, encontrar el número de neuronas ocultas con las que se consigue el mejor accuracy\n",
    "  optimal_hidden_layer_size = np.argmax(arr_accuracy)+1 # +1 porque np.argmax() nos proporciona la posición en la lista del mejor valor. Las posiciones empiezan en 0 y nosotros empezamos en 1 neurona\n",
    "  print(\"\\n**** El número de neuronas de la capa oculta óptimo es: \", optimal_hidden_layer_size)\n",
    "  print(\"**** Con esas neuronas en la capa oculta, el accuracy del conjunto de validación es: \", max(arr_accuracy))\n",
    "\n",
    "  return optimal_hidden_layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_DIBz9I-d5W"
   },
   "source": [
    "# B) EJECUCIÓN DEL PROBLEMA BUSCANDO EL NÚMERO ÓPTIMO DE NEURONAS DE LA CAPA OCULTA Y PREDICIENDO SOBRE EL CONJUNTO DE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gcQTttHKkED",
    "outputId": "fb3e0a07-428f-4b23-da28-6b7c45a033c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando número óptimo de neuronas de la capa oculta... \n",
      "\n",
      "\n",
      "Número máximo de neuronas:  10\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  1\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.063616\n",
      "         Iterations: 30\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 118\n",
      "accuracy:  0.978\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  2\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.010105\n",
      "         Iterations: 30\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 119\n",
      "accuracy:  0.983\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  3\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000999\n",
      "         Iterations: 30\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 105\n",
      "accuracy:  0.978\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  4\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.042750\n",
      "         Iterations: 30\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 107\n",
      "accuracy:  0.982\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  5\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000606\n",
      "         Iterations: 30\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 113\n",
      "accuracy:  0.983\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  6\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.001151\n",
      "         Iterations: 30\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 98\n",
      "accuracy:  0.978\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  7\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000493\n",
      "         Iterations: 30\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 112\n",
      "accuracy:  0.982\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  8\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000835\n",
      "         Iterations: 30\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 110\n",
      "accuracy:  0.98\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  9\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000374\n",
      "         Iterations: 30\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 107\n",
      "accuracy:  0.98\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  10\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000291\n",
      "         Iterations: 30\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 111\n",
      "accuracy:  0.978\n",
      "\n",
      "**** El número de neuronas de la capa oculta óptimo es:  2\n",
      "**** Con esas neuronas en la capa oculta, el accuracy del conjunto de validación es:  0.983\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.009307\n",
      "         Iterations: 30\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 110\n",
      "Accuracy del conjunto de test:  0.99\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Inicializar el tamaño de la capa de entrada y el número de clases del problema\n",
    "input_layer_size = 1899 # Número de columnas de los datos de entrada (X)\n",
    "num_labels = 1 # Spam/No spam\n",
    "\n",
    "# Paso 2: Calcular el número óptimo de neuronas en la capa oculta usando la función optimalHiddenNeurons\n",
    "optimal_hidden_layer_size = optimalHiddenNeurons(input_layer_size, num_labels, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Paso 3: Inicialización de pesos con el número óptimo de neuronas en la capa oculta\n",
    "initial_theta1 = randInitializeWeights(input_layer_size, optimal_hidden_layer_size)\n",
    "initial_theta2 = randInitializeWeights(optimal_hidden_layer_size, num_labels)\n",
    "\n",
    "# Paso 4: Entrenamiento de la red neuronal con el número de neuronas óptimo y obtener los pesos óptimos. \n",
    "# Recordar que el entrenamiento debe hacerse con el conjunto de training entero (X_train y X_val)\n",
    "X_train_completo = ... # IMPORTANTE\n",
    "y_train_completo = ... # IMPORTANTE\n",
    "theta1_opt, theta2_opt = training(initial_theta1, initial_theta2, X_train_completo, y_train_completo, input_layer_size, optimal_hidden_layer_size, num_labels)\n",
    "\n",
    "# Paso 5: Predecir usando el conjunto de test y calcular el error\n",
    "pred = predict(theta1_opt, theta2_opt, ... )\n",
    "print(\"Accuracy del conjunto de test: \", np.mean(pred== .... )) # Calcular la precisión/accuracy máxima"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
